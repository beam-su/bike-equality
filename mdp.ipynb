{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import h3\n",
    "from shapely.geometry import Point\n",
    "from mdptoolbox.mdp import ValueIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the hexagonal geojson file\n",
    "hex_gdf = gpd.read_file(\"geojson\\\\hex_dataset.geojson\")\n",
    "\n",
    "# Ensure necessary columns exist\n",
    "if \"avg_imd_score\" not in hex_gdf.columns or \"total_population\" not in hex_gdf.columns:\n",
    "    raise ValueError(\"Missing column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare states and num_states\n",
    "hex_indices = hex_gdf[\"h3_index\"]\n",
    "states = list(hex_indices)\n",
    "num_states = len(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing stations\n",
    "existing_stations_gdf = gpd.read_file(\"geojson\\\\existing_bike_stations.geojson\")\n",
    "\n",
    "if hex_gdf.crs != existing_stations_gdf.crs:\n",
    "    existing_stations_gdf = existing_stations_gdf.to_crs(hex_gdf.crs)\n",
    "\n",
    "existing_station_coords = existing_stations_gdf.geometry.apply(lambda geom: (geom.y, geom.x))\n",
    "resolution = h3.get_resolution(states[0])\n",
    "existing_station_hexes = set(h3.latlng_to_cell(lat, lon, resolution) for lat, lon in existing_station_coords)\n",
    "\n",
    "# Build a reverse lookup for station hexes\n",
    "station_density_lookup = {}\n",
    "\n",
    "for h in states:\n",
    "    neighbors = h3.grid_disk(h, 1)\n",
    "    count = sum(1 for n in neighbors if n in existing_station_hexes)\n",
    "    station_density_lookup[h] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an adjacency graph based on H3 neighbors\n",
    "G = nx.Graph()\n",
    "hex_indices = hex_gdf[\"h3_index\"]\n",
    "hex_indices_values = hex_indices.values  # for faster lookup\n",
    "\n",
    "for h in hex_indices:\n",
    "    neighbors = h3.grid_disk(h, 1)\n",
    "    for n in neighbors:\n",
    "        if n in hex_indices_values:\n",
    "            G.add_edge(h, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise rewards\n",
    "rewards = np.zeros(num_states)\n",
    "\n",
    "# Extract population and IMD arrays for normalization\n",
    "all_pop = hex_gdf[\"total_population\"]\n",
    "all_imd = hex_gdf[\"avg_imd_score\"]\n",
    "\n",
    "min_pop, max_pop = all_pop.min(), all_pop.max()\n",
    "min_imd, max_imd = all_imd.min(), all_imd.max()\n",
    "\n",
    "# Tunable weights for demand vs equity\n",
    "alpha = 0.5  # weight for population\n",
    "beta = 0.5   # weight for deprivation (equity)\n",
    "gamma = 0.4 # Station Density Weight\n",
    "\n",
    "max_station_density = max(station_density_lookup.values()) or 1 # avoid division by zero\n",
    "\n",
    "for i, h in enumerate(states):\n",
    "    row = hex_gdf[hex_gdf[\"h3_index\"] == h]\n",
    "    if not row.empty:\n",
    "        imd_score = row[\"avg_imd_score\"].values[0]\n",
    "        population = row[\"total_population\"].values[0]\n",
    "\n",
    "        norm_pop = (population - min_pop) / (max_pop - min_pop)\n",
    "        norm_imd = (imd_score - min_imd) / (max_imd - min_imd)\n",
    "\n",
    "        # Less dense areas get higher rewards\n",
    "        station_density = station_density_lookup[h]\n",
    "        norm_inverse_density = 1 - (station_density / gamma)\n",
    "\n",
    "        reward = (\n",
    "            alpha * norm_pop\n",
    "            + beta * norm_imd\n",
    "            + gamma * norm_inverse_density\n",
    "        )\n",
    "\n",
    "        rewards[i] = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 2\n",
    "transition_matrix = np.zeros((num_actions, num_states, num_states))\n",
    "\n",
    "# Action 0: Do nothing\n",
    "for i in range(num_states):\n",
    "    transition_matrix[0, i, i] = 1.0\n",
    "\n",
    "# Action 1: Place a station\n",
    "for i, h in enumerate(states):\n",
    "    neighbors = h3.grid_disk(h, 1)\n",
    "    valid_neighbors = [n for n in neighbors if n in states]\n",
    "    prob = 1 / len(valid_neighbors) if valid_neighbors else 0\n",
    "    for n in valid_neighbors:\n",
    "        j = states.index(n)\n",
    "        transition_matrix[1, i, j] = prob\n",
    "    transition_matrix[1, i, i] += 1.0  # self-loop\n",
    "\n",
    "# Normalize transitions\n",
    "for a in range(num_actions):\n",
    "    for s in range(num_states):\n",
    "        row_sum = np.sum(transition_matrix[a, s, :])\n",
    "        if row_sum > 0:\n",
    "            transition_matrix[a, s, :] /= row_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_hexes = set()\n",
    "for h in existing_station_hexes:\n",
    "    allowed_hexes.update(h3.grid_disk(h, 3))  # within 3 rings (525m, which was chosen because people are only willing to walk around 400-500m from a bike station to their destination https://www.welovecycling.com/wide/2020/04/30/how-far-are-you-willing-to-walk-for-bike-sharing/)\n",
    "\n",
    "filtered_states = [h for h in states if h in allowed_hexes]\n",
    "\n",
    "if not filtered_states:\n",
    "    raise ValueError(\"No candidate hexagons remain after filtering.\")\n",
    "\n",
    "filtered_indices = [states.index(h) for h in filtered_states]\n",
    "filtered_rewards = rewards[filtered_indices]\n",
    "filtered_transition_matrix = transition_matrix[:, filtered_indices, :][:, :, filtered_indices]\n",
    "\n",
    "# Re-normalize transition matrix after slicing\n",
    "for a in range(filtered_transition_matrix.shape[0]):\n",
    "    for s in range(filtered_transition_matrix.shape[1]):\n",
    "        row_sum = np.sum(filtered_transition_matrix[a, s, :])\n",
    "        if row_sum > 0:\n",
    "            filtered_transition_matrix[a, s, :] /= row_sum\n",
    "        else:\n",
    "            # self-loop if no transitions\n",
    "            filtered_transition_matrix[a, s, s] = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = ValueIteration(filtered_transition_matrix, filtered_rewards, discount=0.9)\n",
    "mdp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'best_hexagons.geojson' and 'best_hexagons.csv'\n"
     ]
    }
   ],
   "source": [
    "best_hex_indices = np.argsort(mdp.V)[-5:]\n",
    "best_hexes = [filtered_states[i] for i in best_hex_indices]\n",
    "\n",
    "best_locations = hex_gdf[hex_gdf[\"h3_index\"].isin(best_hexes)][\n",
    "    [\"h3_index\", \"geometry\", \"total_population\", \"avg_imd_score\"]\n",
    "]\n",
    "\n",
    "best_locations.to_file(\"Results\\\\best_hexagons.geojson\", driver=\"GeoJSON\")\n",
    "best_locations.drop(columns=\"geometry\").to_csv(\"Results\\\\best_hexagons.csv\", index=False)\n",
    "\n",
    "print(\"Results saved to 'best_hexagons.geojson' and 'best_hexagons.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
