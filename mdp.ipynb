{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H3-Based MDP\n",
    "Note that in the code we use $\\gamma$ to represent the scaler for the station density reward function. However, the reason in the paper we represent it as $\\delta$ is so that its not confusing to separate the $\\delta$ in the context of the reward function or the $\\delta$ used as a discount factor for Value Iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import h3\n",
    "from shapely.geometry import Point\n",
    "from mdptoolbox.mdp import ValueIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the hexagonal geojson file\n",
    "hex_gdf = gpd.read_file(\"geojson\\\\hex_dataset.geojson\")\n",
    "\n",
    "# Ensure necessary columns exist\n",
    "if \"avg_imd_score\" not in hex_gdf.columns or \"total_population\" not in hex_gdf.columns:\n",
    "    raise ValueError(\"Missing column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare states and num_states\n",
    "hex_indices = hex_gdf[\"h3_index\"]\n",
    "states = list(hex_indices)\n",
    "num_states = len(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing stations\n",
    "existing_stations_gdf = gpd.read_file(\"geojson\\\\existing_bike_stations.geojson\")\n",
    "\n",
    "if hex_gdf.crs != existing_stations_gdf.crs:\n",
    "    existing_stations_gdf = existing_stations_gdf.to_crs(hex_gdf.crs)\n",
    "\n",
    "existing_station_coords = existing_stations_gdf.geometry.apply(lambda geom: (geom.y, geom.x))\n",
    "resolution = h3.get_resolution(states[0])\n",
    "existing_station_hexes = set(h3.latlng_to_cell(lat, lon, resolution) for lat, lon in existing_station_coords)\n",
    "\n",
    "# Build a reverse lookup for station hexes\n",
    "station_density_lookup = {}\n",
    "\n",
    "for h in states:\n",
    "    neighbors = h3.grid_disk(h, 1)\n",
    "    count = sum(1 for n in neighbors if n in existing_station_hexes)\n",
    "    station_density_lookup[h] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an adjacency graph based on H3 neighbors\n",
    "G = nx.Graph()\n",
    "hex_indices = hex_gdf[\"h3_index\"]\n",
    "hex_indices_values = hex_indices.values  # for faster lookup\n",
    "\n",
    "for h in hex_indices:\n",
    "    neighbors = h3.grid_disk(h, 1)\n",
    "    for n in neighbors:\n",
    "        if n in hex_indices_values:\n",
    "            G.add_edge(h, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise rewards\n",
    "rewards = np.zeros(num_states)\n",
    "\n",
    "# Extract population and IMD arrays for normalization\n",
    "all_pop = hex_gdf[\"total_population\"]\n",
    "all_imd = hex_gdf[\"avg_imd_score\"]\n",
    "\n",
    "min_pop, max_pop = all_pop.min(), all_pop.max()\n",
    "min_imd, max_imd = all_imd.min(), all_imd.max()\n",
    "\n",
    "# Tunable weights for demand vs equity\n",
    "alpha = 0.5  # weight for population\n",
    "beta = 0.5   # weight for deprivation (equity)\n",
    "gamma = 0.4 # Station Density Weight\n",
    "\n",
    "max_station_density = max(station_density_lookup.values()) or 1 # avoid division by zero\n",
    "\n",
    "for i, h in enumerate(states):\n",
    "    row = hex_gdf[hex_gdf[\"h3_index\"] == h]\n",
    "    if not row.empty:\n",
    "        imd_score = row[\"avg_imd_score\"].values[0]\n",
    "        population = row[\"total_population\"].values[0]\n",
    "\n",
    "        norm_pop = (population - min_pop) / (max_pop - min_pop)\n",
    "        norm_imd = (imd_score - min_imd) / (max_imd - min_imd)\n",
    "\n",
    "        # Less dense areas get higher rewards\n",
    "        station_density = station_density_lookup[h]\n",
    "        norm_inverse_density = 1 - (station_density / gamma)\n",
    "\n",
    "        reward = (\n",
    "            alpha * norm_pop\n",
    "            + beta * norm_imd\n",
    "            + gamma * norm_inverse_density\n",
    "        )\n",
    "\n",
    "        rewards[i] = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 2\n",
    "transition_matrix = np.zeros((num_actions, num_states, num_states))\n",
    "\n",
    "# Action 0: Do nothing\n",
    "for i in range(num_states):\n",
    "    transition_matrix[0, i, i] = 1.0\n",
    "\n",
    "# Action 1: Place a station\n",
    "for i, h in enumerate(states):\n",
    "    neighbors = h3.grid_disk(h, 1)\n",
    "    valid_neighbors = [n for n in neighbors if n in states]\n",
    "    prob = 1 / len(valid_neighbors) if valid_neighbors else 0\n",
    "    for n in valid_neighbors:\n",
    "        j = states.index(n)\n",
    "        transition_matrix[1, i, j] = prob\n",
    "    transition_matrix[1, i, i] += 1.0  # self-loop\n",
    "\n",
    "# Normalize transitions\n",
    "for a in range(num_actions):\n",
    "    for s in range(num_states):\n",
    "        row_sum = np.sum(transition_matrix[a, s, :])\n",
    "        if row_sum > 0:\n",
    "            transition_matrix[a, s, :] /= row_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_hexes = set()\n",
    "for h in existing_station_hexes:\n",
    "    allowed_hexes.update(h3.grid_disk(h, 3))  # within 3 rings (525m, which was chosen because people are only willing to walk around 400-500m from a bike station to their destination https://www.welovecycling.com/wide/2020/04/30/how-far-are-you-willing-to-walk-for-bike-sharing/)\n",
    "\n",
    "filtered_states = [h for h in states if h in allowed_hexes]\n",
    "\n",
    "if not filtered_states:\n",
    "    raise ValueError(\"No candidate hexagons remain after filtering.\")\n",
    "\n",
    "filtered_indices = [states.index(h) for h in filtered_states]\n",
    "filtered_rewards = rewards[filtered_indices]\n",
    "filtered_transition_matrix = transition_matrix[:, filtered_indices, :][:, :, filtered_indices]\n",
    "\n",
    "# Re-normalize transition matrix after slicing\n",
    "for a in range(filtered_transition_matrix.shape[0]):\n",
    "    for s in range(filtered_transition_matrix.shape[1]):\n",
    "        row_sum = np.sum(filtered_transition_matrix[a, s, :])\n",
    "        if row_sum > 0:\n",
    "            filtered_transition_matrix[a, s, :] /= row_sum\n",
    "        else:\n",
    "            # self-loop if no transitions\n",
    "            filtered_transition_matrix[a, s, s] = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = ValueIteration(filtered_transition_matrix, filtered_rewards, discount=0.9)\n",
    "mdp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hex_indices = np.argsort(mdp.V)[-5:]\n",
    "best_hexes = [filtered_states[i] for i in best_hex_indices]\n",
    "\n",
    "best_locations = hex_gdf[hex_gdf[\"h3_index\"].isin(best_hexes)][\n",
    "    [\"h3_index\", \"geometry\", \"total_population\", \"avg_imd_score\"]\n",
    "]\n",
    "\n",
    "best_locations.to_file(\"Results\\\\best_hexagons.geojson\", driver=\"GeoJSON\")\n",
    "best_locations.drop(columns=\"geometry\").to_csv(\"Results\\\\best_hexagons.csv\", index=False)\n",
    "\n",
    "print(\"Results saved to 'best_hexagons.geojson' and 'best_hexagons.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voronoi MDP\n",
    "One difference in methdology for the Voronoi-based MDP is that we need to do greedy selection post-processing to prevent the stations from clustering together. This is because the Voronoi cells significantly vary in size and shape, unlike H3 hexagons that are uniform and regularly spaced, which naturally have spatial separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.neighbors import BallTree\n",
    "import mdptoolbox\n",
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and normalise data\n",
    "candidate_df = pd.read_csv(\"Dataset\\\\voronoi_candidates.csv\")\n",
    "\n",
    "candidate_df['norm_pop'] = (candidate_df['Population'] - candidate_df['Population'].min()) / (candidate_df['Population'].max() - candidate_df['Population'].min())\n",
    "candidate_df['norm_imd'] = (candidate_df['IMD Score'] - candidate_df['IMD Score'].min()) / (candidate_df['IMD Score'].max() - candidate_df['IMD Score'].min())\n",
    "candidate_df['norm_density'] = candidate_df['Density Weight'] / candidate_df['Density Weight'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same weights as h3-based MDP\n",
    "alpha = 0.5\n",
    "beta = 0.5\n",
    "gamma = 0.4\n",
    "\n",
    "reward_base = (\n",
    "    alpha * candidate_df['norm_pop'] +\n",
    "    beta * candidate_df['norm_imd'] -\n",
    "    gamma * candidate_df['norm_density']\n",
    ").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph from candidate location (r=500m)\n",
    "coords_rad = np.deg2rad(candidate_df[['lat', 'lon']].values)\n",
    "tree = BallTree(coords_rad, metric='haversine')\n",
    "G = nx.Graph()\n",
    "for i, coord in enumerate(coords_rad):\n",
    "    indices = tree.query_radius([coord], r=500 / 6371000)[0]\n",
    "    for j in indices:\n",
    "        if i != j:\n",
    "            G.add_edge(i, j)\n",
    "neighbors = {i: list(G.neighbors(i)) for i in range(len(candidate_df))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MDP\n",
    "n = len(candidate_df)\n",
    "A = 2  # 0 = don't build, 1 = build\n",
    "P = [np.eye(n) for _ in range(A)]  # identity transitions\n",
    "\n",
    "R = np.zeros((n, A))\n",
    "R[:, 0] = 0\n",
    "R[:, 1] = reward_base.copy()\n",
    "for i in range(n):\n",
    "    if neighbors[i]:\n",
    "        neighbor_penalty = np.mean([reward_base[j] for j in neighbors[i]])\n",
    "        R[i, 1] -= 0.3 * neighbor_penalty * len(neighbors[i]) / max(len(nbrs) for nbrs in neighbors.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi = mdptoolbox.mdp.ValueIteration(P, R, discount=0.9)\n",
    "vi.run()\n",
    "\n",
    "candidate_df['Value'] = vi.V\n",
    "candidate_df['Policy'] = vi.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy post-processing to prevent spatial clustering\n",
    "\n",
    "num_stations_to_build = 20\n",
    "min_distance_m = 500\n",
    "policy_df = candidate_df[candidate_df[\"Policy\"] == 1].copy()\n",
    "policy_df = policy_df.sort_values(\"Value\", ascending=False)\n",
    "\n",
    "selected_stations = []\n",
    "for _, row in policy_df.iterrows():\n",
    "    current_point = (row[\"lat\"], row[\"lon\"])\n",
    "    too_close = False\n",
    "    for sel in selected_stations:\n",
    "        sel_point = (sel[\"lat\"], sel[\"lon\"])\n",
    "        if geodesic(current_point, sel_point).meters < min_distance_m:\n",
    "            too_close = True\n",
    "            break\n",
    "    if not too_close:\n",
    "        selected_stations.append(row)\n",
    "    if len(selected_stations) == num_stations_to_build:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            lat       lon     Value  build_rank\n",
      "1073  51.526790 -0.168740  7.728729           1\n",
      "771   51.535840 -0.034040  7.222046           2\n",
      "4658  51.520940 -0.177510  7.067878           3\n",
      "5664  51.545976 -0.017493  6.807980           4\n",
      "4909  51.515960 -0.036150  6.703326           5\n",
      "4966  51.518290 -0.024390  6.492273           6\n",
      "2534  51.520160 -0.062090  6.317756           7\n",
      "1731  51.531670 -0.057770  6.243930           8\n",
      "3973  51.535220 -0.074180  5.996592           9\n",
      "3544  51.520450 -0.185960  5.982743          10\n",
      "1686  51.533090 -0.064850  5.982435          11\n",
      "2152  51.535190 -0.137040  5.981712          12\n",
      "5798  51.532229 -0.084802  5.925009          13\n",
      "3920  51.497970 -0.076330  5.923527          14\n",
      "5543  51.512870 -0.057790  5.877344          15\n",
      "5420  51.515910 -0.045700  5.837042          16\n",
      "5795  51.533170 -0.096540  5.827771          17\n",
      "1566  51.528030 -0.095260  5.804530          18\n",
      "4951  51.523860 -0.024200  5.791032          19\n",
      "3493  51.535240 -0.110940  5.775788          20\n"
     ]
    }
   ],
   "source": [
    "build_df = pd.DataFrame(selected_stations)\n",
    "build_df[\"build_rank\"] = np.arange(1, len(build_df) + 1)\n",
    "build_df.to_csv(\"station_impact_ranking.csv\", index=False)\n",
    "print(build_df[[\"lat\", \"lon\", \"Value\", \"build_rank\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
